# -*- coding: utf-8 -*-
"""
[ì—­í• ]
- ëŒ€ë²•ì› í¬í„¸(https://portal.scourt.go.kr)ì—ì„œ íŒê²°ë¬¸ ëª©ë¡/ìƒì„¸ë¥¼ ê°€ì ¸ì™€ raw JSONLë¡œ ì €ì¥í•œë‹¤.
- RAG ì›ì²œ ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„.

[ì…ì¶œë ¥]
- ì¶œë ¥: data/raw/{keyword}_cases.jsonl
  (ê° ì¤„: {"case_uid":..., "case_no":..., "summary":..., "reason":..., "xml":..., "_detail":"hit|miss", ...})

[í•µì‹¬ íë¦„]
1) ê²€ìƒ‰ ëª©ë¡ API í˜¸ì¶œ(í‚¤ì›Œë“œ, í˜ì´ì§€ë„¤ì´ì…˜)
2) ê° í•­ëª©ì˜ ìƒì„¸ API/í˜ì´ì§€ë¥¼ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ì‹œë„(Ctxt/Dtl/Ctt, POST/GET/í˜ì´ì§€ìŠ¤í¬ë©)
3) ìš”ì•½/ì´ìœ /ì „ë¬¸(HTML)ì„ í…ìŠ¤íŠ¸í™”í•˜ì—¬ ì €ì¥

[ì–¸ì œ ë‹¤ì‹œ ì“°ë‚˜?]
- ìƒˆë¡œìš´ í‚¤ì›Œë“œë‚˜ ê¸°ê°„ìœ¼ë¡œ 'ì¶”ê°€ ìˆ˜ì§‘'ì´ í•„ìš”í•  ë•Œ.
- ê¸°ì¡´ ë°ì´í„°ê°€ ë„ˆë¬´ ì˜¤ë˜ë˜ì–´ ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•  ë•Œ.

[ì˜ì¡´/ì£¼ì˜]
- requests + BeautifulSoup í•„ìš”.
- í¬í„¸ì˜ ì‘ë‹µ í¬ë§·/ë³´ì•ˆì •ì±…ì´ ë°”ë€Œë©´ ì—”ë“œí¬ì¸íŠ¸/í—¤ë”/í˜ì´ë¡œë“œë¥¼ ì¡°ì •í•´ì•¼ í•¨.
- ê³¼ë„í•œ ìš”ì²­ ë°©ì§€ë¥¼ ìœ„í•´ delay_ms íŠœë‹ ê°€ëŠ¥.
"""



# ingestion/scourt_crawler.py
import time, random, json, requests, datetime, re
from pathlib import Path
from html import unescape
from bs4 import BeautifulSoup

BASE = "https://portal.scourt.go.kr"

# ğŸ” ëª©ë¡ API
SEARCH_URL = f"{BASE}/pgp/pgp1011/selectJdcpctSrchRsltLst.on"

# ğŸ“„ ìƒì„¸ API (ìš°ì„ ìˆœìœ„: Ctxt â†’ Dtl â†’ (êµ¬í˜•)Ctt)
DETAIL_URLS = [
    f"{BASE}/pgp/pgp1011/selectJdcpctCtxt.on",               # âœ… ì „ë¬¸ HTML(orgdocXmlCtt)
    f"{BASE}/pgp/pgp1011/selectJdcpctDtl.on",                # ë©”íƒ€(ì‚¬ê±´ë²ˆí˜¸/ì„ ê³ ì¼ ë“±)
    f"{BASE}/pgp/pgp1011/PGP1011M04/selectJdcpctCtt.on",     # ì¼ë¶€ í™˜ê²½ì—ì„œ ì“°ëŠ” êµ¬í˜•/ë‚´ë¶€ ì—”ë“œí¬ì¸íŠ¸
]

# ğŸ” ìƒì„¸ í˜ì´ì§€(ì§ì ‘ HTML ìŠ¤í¬ë©) í´ë°±
DETAIL_PAGES = [
    f"{BASE}/pgp/pgp1011/selectJdcpctCttPage.on",
    f"{BASE}/pgp/pgp1011/PGP1011M04/selectJdcpctCttPage.on",
]
DETAIL_REFERER_TPL = f"{BASE}/pgp/pgp1011/selectJdcpctCttPage.on?jisCntntsSrno={{uid}}"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36",
    "Accept": "application/json, text/plain, */*",
    "Content-Type": "application/json; charset=UTF-8",
    "Origin": BASE,
    "Referer": f"{BASE}/pgp/pgp1011/selectJdcpctSrchRqstPage.on",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    # ì‹¤ë¬´ì—ì„œ ì¢…ì¢… ìš”êµ¬ë¨ (ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸)
    "SC-Pgmid": "PGP1011M04",
}

RAW_DIR = Path("data/raw")
RAW_DIR.mkdir(parents=True, exist_ok=True)

def build_body(keyword, page_no=1, page_size=20):
    return {
        "dma_searchParam": {
            "srchwd": keyword,
            "sort": "jis_jdcpc_instn_dvs_cd_s asc, $relevance desc, prnjdg_ymd_o desc, jdcpct_gr_cd_s asc",
            "sortType": "ì •í™•ë„",
            "pageNo": str(page_no),
            "pageSize": str(page_size),
            # ê²€ìƒ‰ ê·¸ë£¹ì½”ë“œ(ëŒ€ë²•ì› ê°„í–‰/ì „ì›í•©ì˜ì²´ ë“±) â€“ í•„ìš”ì‹œ ì¡°ì •
            "jdcpctGrCd": "111|112|130|141|180|182|232|235|201",
            "category": "jdcpct",
            "isKwdSearch": "N",
        }
    }

def _dump_debug(content, suffix):
    ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    p = RAW_DIR / f"_resp_{ts}.{suffix}"
    if isinstance(content, (dict, list)):
        p.write_text(json.dumps(content, ensure_ascii=False, indent=2), encoding="utf-8")
    else:
        p.write_text(str(content), encoding="utf-8")
    return str(p)

def strip_html(s: str) -> str:
    if not s:
        return ""
    s = unescape(s)
    txt = BeautifulSoup(s, "html.parser").get_text(" ", strip=True)
    return re.sub(r"\s+", " ", txt).strip()

def _detail_payload_variants(uid: str):
    # ì‹¤ì œë¡œëŠ” Ctxt/Dtl ëª¨ë‘ {"dma_searchParam":{"jisCntntsSrno": uid}} í•œ ë°©ì´ë©´ ì¶©ë¶„
    return [
        {"dma_searchParam": {"jisCntntsSrno": uid}},
        {"dma_searchParam": {"jisCntntsSrno": uid, "jdcpctUid": uid}},
        {"dma_detailParam": {"jisCntntsSrno": uid}},
        {"jisCntntsSrno": uid},
    ]

def _pick_first(d: dict, keys: list[str]) -> str:
    for k in keys:
        v = d.get(k)
        if isinstance(v, str) and v.strip():
            return v
    return ""

def _parse_detail_json(payload: dict) -> dict:
    """
    JSON ì‘ë‹µì—ì„œ ìš”ì•½/ì´ìœ /ì „ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€í•œ ë½‘ì•„ëƒ„.
    *ì „ë¬¸ HTMLì€ Ctxt ì‘ë‹µì˜ orgdocXmlCttì— ìˆìŒ*
    """
    node = payload.get("result") or payload.get("data") or payload
    candidates = [node]
    if isinstance(node, dict):
        candidates += [v for v in node.values() if isinstance(v, dict)]

    summary = reason = xml = ""
    for n in candidates:
        if not isinstance(n, dict):
            continue
        # ìš”ì•½
        summary = summary or _pick_first(
            n, ["jdcpctSumrCtt", "sumry", "summary", "intdCtt", "sumCtt", "smrCtt"]
        )
        # ì´ìœ /ê²°ë¡ 
        reason = reason or _pick_first(
            n, ["dcdcsCtt", "reason", "rcnCtt", "rsnCtt", "mainCtt", "jdgCtt"]
        )
        # ë³¸ë¬¸: Ctxt ì‘ë‹µì€ orgdocXmlCtt, ê³¼ê±°ì—” jdcpctXmlCtt ë“±ë„ ì¡´ì¬
        xml = xml or _pick_first(
            n, ["orgdocXmlCtt", "jdcpctXmlCtt", "xml", "cnCtt", "cntntsCtt"]
        )

    return {
        "full_summary": strip_html(summary),
        "full_reason": strip_html(reason),
        "full_xml": strip_html(xml),  # ì›ë¬¸ HTMLì„ í…ìŠ¤íŠ¸í™”(í•„ìš”ì‹œ HTML ê·¸ëŒ€ë¡œ ì €ì¥í•˜ë„ë¡ ë°”ê¿”ë„ ë¨)
    }

def _parse_detail_html(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")

    def pick_text(*selectors):
        for sel in selectors:
            el = soup.select_one(sel)
            if el:
                return strip_html(el.get_text(" ", strip=True))
        return ""

    return {
        "full_summary": pick_text("#jdcpctSumrCtt", ".sumr", "#summary", ".summary", "[data-field='summary']"),
        "full_reason": pick_text("#dcdcsCtt", ".reason", "#decision", ".dcdcs", "[data-field='reason']"),
        "full_xml": pick_text("#jdcpctXmlCtt", ".xml", "#content", ".content", ".viewArea", "[data-field='xml']"),
    }

def _csrf_header(sess: requests.Session) -> dict:
    token = (
        sess.cookies.get("XSRF-TOKEN")
        or sess.cookies.get("CSRF-TOKEN")
        or sess.cookies.get("csrfToken")
    )
    return {"X-CSRF-TOKEN": token} if token else {}

def _submission_for(url: str) -> str:
    """
    í˜ì´ì§€ ë‚´ë¶€ì—ì„œ ì“°ëŠ” submissionid í—¤ë”. ì—†ì–´ë„ ë™ì‘í•˜ë‚˜, ì„±ê³µë¥ /ì•ˆì •ì„±â†‘
    """
    if url.endswith("selectJdcpctCtxt.on"):
        return "mf_wfm_pgpDtlMain_sbm_selectJdcpctCtxt"
    if url.endswith("selectJdcpctDtl.on"):
        return "mf_wfm_pgpDtlMain_sbm_selectJdcpctDtl"
    if url.endswith("selectJdcpctCtt.on"):
        return "mf_wfm_pgpDtlMain_sbm_selectJdcpctCtt"
    return ""

def _try_detail_post_json(sess: requests.Session, url: str, uid: str):
    headers = {
        **HEADERS,
        "X-Requested-With": "XMLHttpRequest",
        "Referer": DETAIL_REFERER_TPL.format(uid=uid),
        "Accept": "application/json, text/html, */*",
        "Content-Type": "application/json; charset=UTF-8",
        **_csrf_header(sess),
    }
    sub = _submission_for(url)
    if sub:
        headers["submissionid"] = sub

    for payload in _detail_payload_variants(uid):
        try:
            r = sess.post(url, headers=headers, json=payload, timeout=20, allow_redirects=True)
            if r.status_code >= 400:
                continue
            ctype = (r.headers.get("Content-Type") or "").lower()
            if "json" in ctype or r.text.strip().startswith("{"):
                try:
                    return _parse_detail_json(r.json())
                except Exception:
                    pass
            return _parse_detail_html(r.text)
        except Exception:
            continue
    return None

def _try_detail_post_form(sess: requests.Session, url: str, uid: str):
    headers = {
        **HEADERS,
        "X-Requested-With": "XMLHttpRequest",
        "Referer": DETAIL_REFERER_TPL.format(uid=uid),
        "Accept": "text/html,application/json,*/*",
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        **_csrf_header(sess),
    }
    sub = _submission_for(url)
    if sub:
        headers["submissionid"] = sub

    for payload in _detail_payload_variants(uid):
        flat = {}
        for k, v in payload.items():
            if isinstance(v, dict):
                flat.update(v)
            else:
                flat[k] = v
        try:
            r = sess.post(url, headers=headers, data=flat, timeout=20, allow_redirects=True)
            if r.status_code >= 400:
                continue
            ctype = (r.headers.get("Content-Type") or "").lower()
            if "json" in ctype:
                try:
                    return _parse_detail_json(r.json())
                except Exception:
                    pass
            return _parse_detail_html(r.text)
        except Exception:
            continue
    return None

def _try_detail_get(sess: requests.Session, url: str, uid: str):
    headers = {
        **HEADERS,
        "X-Requested-With": "XMLHttpRequest",
        "Referer": DETAIL_REFERER_TPL.format(uid=uid),
        "Accept": "text/html,application/json,*/*",
        **_csrf_header(sess),
    }
    sub = _submission_for(url)
    if sub:
        headers["submissionid"] = sub

    params_list = [
        {"jisCntntsSrno": uid},
        {"jisCntntsSrno": uid, "jdcpctUid": uid},
    ]
    for params in params_list:
        try:
            r = sess.get(url, headers=headers, params=params, timeout=20, allow_redirects=True)
            if r.status_code >= 400:
                continue
            ctype = (r.headers.get("Content-Type") or "").lower()
            if "json" in ctype:
                try:
                    return _parse_detail_json(r.json())
                except Exception:
                    pass
            return _parse_detail_html(r.text)
        except Exception:
            continue
    return None

def _try_detail_page(sess: requests.Session, uid: str, debug=False):
    """
    AJAXê°€ ë§‰íˆë©´ ìƒì„¸ í˜ì´ì§€(HTML)ë¥¼ ì§ì ‘ ì—´ì–´ ìŠ¤í¬ë©.
    """
    headers = {
        **HEADERS,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Referer": f"{BASE}/pgp/pgp1011/selectJdcpctSrchRqstPage.on",
    }
    params_list = [
        {"jisCntntsSrno": uid},
        {"jisCntntsSrno": uid, "jdcpctUid": uid},
    ]
    for page_url in DETAIL_PAGES:
        for params in params_list:
            try:
                r = sess.get(page_url, headers=headers, params=params, timeout=20, allow_redirects=True)
                if r.status_code >= 400:
                    continue
                parsed = _parse_detail_html(r.text)
                if any(parsed.values()):
                    return parsed
            except Exception:
                continue
    if debug:
        _dump_debug({"detail_page_failed_for": uid}, "json")
    return None

def fetch_detail(sess: requests.Session, uid: str, debug=False) -> dict:
    """
    uid(=jisCntntsSrno)ë¡œ ìƒì„¸ ì „ë¬¸ì„ ê°€ì ¸ì˜¨ë‹¤.
    1) JSON POST â†’ 2) FORM POST â†’ 3) GET â†’ 4) í˜ì´ì§€ ìŠ¤í¬ë©
    """
    for url in DETAIL_URLS:
        parsed = _try_detail_post_json(sess, url, uid)
        if parsed and any(parsed.values()):
            return parsed
        parsed = _try_detail_post_form(sess, url, uid)
        if parsed and any(parsed.values()):
            return parsed
        parsed = _try_detail_get(sess, url, uid)
        if parsed and any(parsed.values()):
            return parsed

    parsed = _try_detail_page(sess, uid, debug=debug)
    if parsed and any(parsed.values()):
        return parsed

    if debug:
        _dump_debug({"detail_failed_for": uid, "err": "all detail urls/payloads failed"}, "json")
    return {"full_summary": "", "full_reason": "", "full_xml": ""}

def crawl(keyword="ì´í˜¼", max_pages=1, delay_ms=(800, 1600), debug=False, fetch_detail_flag=True):
    results = []
    total_count = 0

    with requests.Session() as sess:
        # ğŸ”¸ ì›Œë°ì—…(ì¿ í‚¤/ì„¸ì…˜ ìƒì„±)
        sess.get(BASE, headers=HEADERS, timeout=10)
        sess.get(f"{BASE}/pgp/pgp1011/selectJdcpctSrchRqstPage.on", headers=HEADERS, timeout=10)

        for page in range(1, max_pages + 1):
            r = sess.post(
                SEARCH_URL,
                headers=HEADERS,
                json=build_body(keyword, page, 20),
                timeout=20,
                allow_redirects=True,
            )
            if r.status_code >= 400:
                if debug: _dump_debug(r.text[:800], "txt")
                break

            ctype = (r.headers.get("Content-Type") or "").lower()
            if "application/json" not in ctype:
                if debug: _dump_debug(r.text, "html")
                break

            try:
                data = r.json()
                if debug: _dump_debug(data, "json")
            except Exception:
                if debug: _dump_debug(r.text[:800], "txt")
                break

            node = data.get("result") or data.get("data") or data

            items = []
            if isinstance(node, dict):
                for key in ("dlt_jdcpctRslt","list","items","rows","resultList","dataList"):
                    v = node.get(key)
                    if isinstance(v, list) and v:
                        items = v
                        break
                total_count = node.get("totalCount") or node.get("total") or total_count

            if not items:
                break

            for it in items:
                uid = it.get("jisCntntsSrno")
                row = {
                    "case_uid": uid,
                    "case_no": it.get("csNoLstCtt"),
                    "title": strip_html(it.get("csNmLstCtt")),
                    "court": it.get("cortNm"),
                    "judgment_date": it.get("prnjdgYmd"),
                    "group": it.get("grpJdcpctGrNm"),
                    "type": it.get("adjdTypNm"),
                    "publish": it.get("jdcpctPublcCtt"),
                    # ëª©ë¡ì—ì„œ ì˜¤ëŠ” ê±´ ëŒ€ë¶€ë¶„ ìš”ì•½/ìš”ì§€(ì¤„ì„í‘œ ê°€ëŠ¥)
                    "summary": strip_html(it.get("jdcpctSumrCtt") or ""),
                    "reason":  strip_html(it.get("dcdcsCtt") or ""),
                    "xml":     strip_html(it.get("jdcpctXmlCtt") or ""),  # ì „ë¬¸ì€ ì•„ë˜ì—ì„œ êµì²´
                    "keyword": keyword,
                    "_detail": "skip",
                }

                if fetch_detail_flag and uid:
                    detail = fetch_detail(sess, uid, debug=debug)
                    hit = False
                    if detail.get("full_summary"):
                        row["summary"] = detail["full_summary"]; hit = True
                    if detail.get("full_reason"):
                        row["reason"]  = detail["full_reason"];  hit = True
                    if detail.get("full_xml"):
                        row["xml"]     = detail["full_xml"];     hit = True
                    row["_detail"] = "hit" if hit else "miss"

                results.append(row)

            time.sleep(random.uniform(delay_ms[0] / 1000, delay_ms[1] / 1000))

    out_path = RAW_DIR / f"{keyword}_cases.jsonl"
    with out_path.open("w", encoding="utf-8") as f:
        for row in results:
            f.write(json.dumps(row, ensure_ascii=False) + "\n")

    return str(out_path), len(results), int(total_count or 0)
