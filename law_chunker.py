# filename: law_chunker.py
import json
import re
import hashlib
from pathlib import Path
from typing import Dict, List, Any

IN_PATH  = Path("out/precedents_cleaned_ì´í˜¼.json")   # ì…ë ¥
OUT_PATH = Path("out/chunks_ì´í˜¼.jsonl")              # ì²­í¬ ì¶œë ¥ (JSONL)
STAT_PATH= Path("out/chunk_stats_ì´í˜¼.json")          # í†µê³„

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) ì„¹ì…˜ ê³ ì • ë§¤í•‘ (ì¼ê´€ì„± ë³´ì¥)
#    * ì„¹ì…˜ì´ ì—†ë”ë¼ë„ ë²ˆí˜¸ëŠ” ë°”ê¾¸ì§€ ì•ŠëŠ”ë‹¤.
#    * í•„ìš”ì‹œ ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ë©´ ì „ì²´ ë°ì´í„°ê°€ ê°™ì€ ì¸ë±ìŠ¤ë¡œ ìœ ì§€ë¨.
SECTION_INDEX: Dict[str, int] = {
    "íŒê²°ìš”ì§€":   0,
    "íŒì‹œì‚¬í•­":   1,
    "ì£¼ë¬¸":      2,
    "íŒë¡€ë‚´ìš©":   4,
    "ì „ë¬¸":      5,
    "ì°¸ì¡°ì¡°ë¬¸":   6,
    "ì°¸ì¡°íŒë¡€":   7,
}
# ì²­í¬ ëŒ€ìƒìœ¼ë¡œ ìš°ì„  ì²˜ë¦¬í•  ì„¹ì…˜ ìš°ì„ ìˆœìœ„(ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬)
SECTION_ORDER: List[str] = ["íŒê²°ìš”ì§€", "íŒì‹œì‚¬í•­", "ì£¼ë¬¸", "íŒë¡€ë‚´ìš©", "ì „ë¬¸", "ì°¸ì¡°ì¡°ë¬¸", "ì°¸ì¡°íŒë¡€"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) ì²­í¬ í¬ê¸° ì •ì±… (í•œê¸€ ê¸°ì¤€ ë¬¸ì ìˆ˜)
MIN_CHARS = 350     # ë„ˆë¬´ ì§§ìœ¼ë©´ ì´ì›ƒ ë³‘í•©
SOFT_MAX  = 1200    # ì´ ê°’ì„ ë„˜ìœ¼ë©´ ë¬¸ì¥ê²½ê³„ë¡œ ë¶„í• 
HARD_MAX  = 1600    # ë¬¸ì¥ í•˜ë‚˜ê°€ ë„ˆë¬´ ê¸¸ë©´ ë¹„ìƒ ë¶„í• (ì ˆëŒ€ ì´ˆê³¼ ê¸ˆì§€)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âš  lookbehind ê¸ˆì§€(3.11+ ì—ëŸ¬) â†’ ì•ˆì „í•œ ë¬¸ì¥ ë¶„ë¦¬ ì •ê·œì‹
SENT_SPLIT = re.compile(r"([\.!?â€¦]|\)|\]|\.)\s+|\n+")
WS = re.compile(r"\s+")

def norm_text(s: str) -> str:
    s = s.replace("\u200b", "").replace("\xa0", " ")
    s = WS.sub(" ", s).strip()
    return s

def split_sentences(text: str) -> List[str]:
    # ë¬¸ì¥ ê²½ê³„ ëŒ€ëµ ë¶„í•  í›„ íŠ¸ë¦¼
    parts = [t.strip() for t in SENT_SPLIT.split(text) if t and t.strip()]
    # ë„ˆë¬´ ì§§ê²Œ ì˜ë¦° ì¡°ê° í•©ì¹˜ê¸° (ex. â€œë‹¤.â€ ê°™ì€ ê¼¬ë¦¬)
    merged: List[str] = []
    buf = ""
    for p in parts:
        if not buf:
            buf = p
            continue
        if len(buf) < 60:   # ê¼¬ë¦¬ë¬¸ì¥ ë°©ì§€
            buf = (buf + " " + p).strip()
        else:
            merged.append(buf)
            buf = p
    if buf:
        merged.append(buf)
    return merged

def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8")).hexdigest()

def make_chunks_from_text(base_meta: Dict[str, Any], section: str, raw: str):
    """
    ì£¼ì–´ì§„ ì„¹ì…˜ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ê¸°ì¤€ìœ¼ë¡œ 350~1200ì ì‚¬ì´ì˜ ì²­í¬ë¡œ ìƒì„±.
    """
    section_idx = SECTION_INDEX.get(section, 99)  # ì•Œ ìˆ˜ ì—†ëŠ” ì„¹ì…˜ì€ 99
    sents = split_sentences(norm_text(raw))
    chunks: List[str] = []
    cur = ""

    def flush():
        nonlocal cur
        if cur and cur.strip():
            chunks.append(cur.strip())
        cur = ""

    for st in sents:
        # ë„ˆë¬´ ê¸´ ë‹¨ì¼ ë¬¸ì¥ì€ ê°•ì œ í•˜ë“œì»·
        if len(st) > HARD_MAX:
            tmp = st
            while len(tmp) > HARD_MAX:
                cut = tmp[:HARD_MAX]
                idx = cut.rfind(" ")
                if idx < MIN_CHARS:
                    idx = HARD_MAX
                chunks.append(tmp[:idx].strip())
                tmp = tmp[idx:].strip()
            if tmp:
                if len(cur) + 1 + len(tmp) <= SOFT_MAX:
                    cur = (cur + " " + tmp).strip() if cur else tmp
                else:
                    flush()
                    cur = tmp
            continue

        if not cur:
            cur = st
            continue

        if len(cur) + 1 + len(st) <= SOFT_MAX:
            cur = f"{cur} {st}"
        else:
            if len(cur) < MIN_CHARS:
                cur = f"{cur} {st}"
                if len(cur) > SOFT_MAX:
                    flush()
                    cur = st
            else:
                flush()
                cur = st

    flush()

    # ë§ˆì§€ë§‰ ì•ˆì „ë§: ì—¬ì „íˆ ë„ˆë¬´ ì§§ì€ ê¼¬ë¦¬ë©´ ì• ì²­í¬ì™€ ë³‘í•©
    if len(chunks) >= 2 and len(chunks[-1]) < 200:
        chunks[-2] = f"{chunks[-2]} {chunks[-1]}".strip()
        chunks.pop()

    results = []
    for local_idx, txt in enumerate(chunks):
        item = dict(base_meta)
        item.update({
            "section": section,
            "section_index": section_idx,
            "chunk_index": local_idx,
            "text": txt,
            "chars": len(txt),
        })
        results.append(item)
    return results

def main():
    IN_PATH.parent.mkdir(parents=True, exist_ok=True)
    OUT_PATH.parent.mkdir(parents=True, exist_ok=True)

    data = json.loads(IN_PATH.read_text(encoding="utf-8"))

    seen_hash = set()
    total_chunks = 0
    total_cases = 0
    by_section = {k: 0 for k in SECTION_INDEX.keys()}
    dropped_empty = 0
    deduped = 0

    with OUT_PATH.open("w", encoding="utf-8") as fout:
        for row in data:
            total_cases += 1
            d = row.get("data", {}) or {}
            uid = str(row.get("id") or row.get("uid") or "")
            case_no = d.get("ì‚¬ê±´ë²ˆí˜¸") or d.get("case_no") or ""
            title = d.get("ì‚¬ê±´ëª…") or row.get("case_name") or ""
            court = d.get("ë²•ì›ëª…") or row.get("court") or ""
            jdate = d.get("ì„ ê³ ì¼ì") or row.get("judgment_date") or ""

            base_meta = {
                "uid": uid,
                "case_no": case_no,
                "title": title,
                "court": court,
                "judgment_date": jdate,
            }

            # ì§€ì •í•œ ORDER ìˆœì„œëŒ€ë¡œ ì„¹ì…˜ì„ í›‘ë˜, ì‹¤ì œ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ì„¹ì…˜ë§Œ ì²˜ë¦¬
            for sec in SECTION_ORDER:
                raw = d.get(sec)
                if not raw:
                    continue
                text = norm_text(str(raw))
                if not text:
                    dropped_empty += 1
                    continue

                # ì„¹ì…˜ë³„ chunk_indexëŠ” ì´ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ 0ë¶€í„°
                made = make_chunks_from_text(base_meta, sec, text)

                # ì¤‘ë³µ ì œê±° í›„ ê¸°ë¡
                for item in made:
                    h = sha1(item["text"])
                    if h in seen_hash:
                        deduped += 1
                        continue
                    seen_hash.add(h)
                    item["sha1"] = h
                    # ê´€ì¸¡/ë””ë²„ê¹… í¸ì˜ë¥¼ ìœ„í•œ ì „ì—­ chunk_id
                    item["chunk_id"] = f"{item['uid']}:{SECTION_INDEX.get(sec,99)}:{item['chunk_index']}"
                    fout.write(json.dumps(item, ensure_ascii=False) + "\n")
                    total_chunks += 1
                    by_section[sec] = by_section.get(sec, 0) + 1

    stats = {
        "cases": total_cases,
        "chunks": total_chunks,
        "by_section": by_section,
        "dropped_empty_sections": dropped_empty,
        "deduped": deduped,
        "policy": {
            "min_chars": MIN_CHARS,
            "soft_max": SOFT_MAX,
            "hard_max": HARD_MAX,
            "section_index_fixed_map": SECTION_INDEX,
            "section_order": SECTION_ORDER,
        },
        "inputs": str(IN_PATH),
        "outputs": str(OUT_PATH),
    }
    STAT_PATH.write_text(json.dumps(stats, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"âœ… ì²­í¬ ì™„ë£Œ: {total_chunks} chunks / {total_cases} cases")
    print(f"ğŸ“Š í†µê³„ ì €ì¥: {STAT_PATH}")

if __name__ == "__main__":
    main()
